# Aether-Thread

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
![Python 3.9+](https://img.shields.io/badge/python-3.9%2B-blue)
![Version 0.3.0](https://img.shields.io/badge/version-0.3.0-success)

**Modern thread-safe Python for the no-GIL era** ğŸš€

A developer-friendly toolkit that makes writing thread-safe Python code effortless. With the GIL becoming optional in Python 3.13+, this is the toolkit you need to safely leverage multi-threading without the headache of debugging race conditions.

## The Problem

For 30 years, the Python GIL protected us from race conditions at the cost of parallelism. Now that the GIL can be disabled, legacy code **breaks** or experiences **data corruption** because it was never actually thread-safe.

**Aether-Thread** solves this by providing:
- âœ¨ **@atomic decorator** â€“ One-line thread safety for methods and functions
- ğŸ”’ **Thread-safe collections** â€“ Smart wrappers that protect lists, dicts, and sets
- ğŸ“Š **Contention monitoring** â€“ Identify performance bottlenecks automatically
- ğŸ” **Static analysis** â€“ Find thread-safety issues before they happen

## Quick Start

### 1. Install

```bash
pip install aether-thread
```

### 2. Protect Your Code with @atomic

```python
from aether import atomic

class BankAccount:
    def __init__(self):
        self.balance = 0

    @atomic  # â† That's it! Thread-safe deposit
    def deposit(self, amount):
        self.balance += amount
    
    @atomic
    def withdraw(self, amount):
        if self.balance >= amount:
            self.balance -= amount
            return True
        return False
```

### 3. Use Thread-Safe Collections

```python
from aether import ThreadSafeDict, ThreadSafeList
import threading

# Works exactly like dict/list, but thread-safe
cache = ThreadSafeDict()
results = ThreadSafeList()

def worker(key, value):
    cache[key] = value
    results.append(value)

# Run with 10 concurrent threads - no race conditions!
threads = [threading.Thread(target=worker, args=(f"key_{i}", i)) for i in range(10)]
for t in threads:
    t.start()
for t in threads:
    t.join()

print(f"Cache size: {len(cache)}")  # Safe!
```

### 4. Monitor Contention

```python
from aether import atomic
from aether.monitor import get_monitor

# Enable monitoring
monitor = get_monitor()
monitor.enable()

# ... run your code ...

# See which locks are hot
monitor.print_report()
```

**Output:**
```
CONTENTION MONITORING REPORT
============================
ğŸ”´ HOT LOCKS (>20% contention):
  my_lock
    Acquisitions: 1000
    Contention rate: 45.2%
    ğŸ’¡ Suggestion: Consider lock-free data structure
```

---

## Core Features

### ğŸ¯ **@atomic & @synchronized Decorators**

One-line thread safety for methods. Automatically handles locking with zero boilerplate.

| Feature | Benefit |
|---------|---------|
| **@atomic** | Flexible synchronization with configurable lock types |
| **@synchronized** | Simple thread-safe methods (RLock by default) |
| **GIL-aware** | Only locks when GIL is disabled (no overhead when GIL is on) |
| **Reentrant** | Uses RLock by default â€“ methods can safely call each other |
| **Zero config** | Works out of the box with sensible defaults |

```python
# @atomic with custom options
@atomic(lock_type=LockType.RLOCK, timeout=5.0)
def critical_operation(self):
    # Automatically synchronized!
    pass

# @synchronized for simple cases
@synchronized
def simple_operation(self):
    # Thread-safe, no config needed
    pass
```

### ğŸ”’ **Thread-Safe Collections**

Drop-in replacements for standard Python collections with automatic fine-grained locking:

- **ThreadSafeList** â€“ List with automatic synchronization
- **ThreadSafeDict** â€“ Dict with automatic synchronization  
- **ThreadSafeSet** â€“ Set with automatic synchronization

```python
# Use just like normal collections
safe_list = ThreadSafeList()
safe_dict = ThreadSafeDict()

# All operations are thread-safe
safe_list.append(1)
safe_dict['key'] = 'value'

# Concurrent access from 100 threads - no worries!
```

### ğŸ“Š **Contention Monitoring**

Real-time visibility into lock contention to identify performance bottlenecks:

```python
from aether.monitor import get_monitor

monitor = get_monitor()
monitor.enable()

# ... run your threaded code ...

# Detailed report
stats = monitor.get_stats()

# Find hot locks
hot_locks = stats.get_hot_locks(threshold=20.0)

# Find slowest locks
slowest = stats.get_slowest_locks(limit=10)

# Print comprehensive report
monitor.print_report()
```

### ğŸ” **Static Analysis with aether-audit**

Prevent thread-safety bugs before they happen. Automatic detection of:

- **Shared Mutable State** â€“ Global variables, class attributes, mutable defaults
- **Race Condition Patterns** â€“ Check-then-act, read-modify-write operations
- **Auto-Fix Suggestions** â€“ @atomic decorators, ThreadSafe wrappers, explicit locks

```python
from aether.audit import audit_code, audit_directory

# Quick analysis of a single file
code = open("myfile.py").read()
report = audit_code(code, "myfile.py")
print(report)

# Comprehensive project scan
report = audit_directory("src/", exclude_dirs=["venv"])
print(report)
```

**Example Detection Output:**

```
ğŸ”´ SHARED STATE ISSUES (1 found):
  Line 4: Global variable 'counter' is mutable and accessible from multiple threads
  â†’ Fix: Use @atomic decorator on functions that modify 'counter'

ğŸŸ¡ RACE CONDITIONS (2 patterns found):
  Line 9: Compound assignment 'counter += ...' is not atomic
  â†’ Fix: Use @atomic decorator
```

[â†’ Full aether-audit documentation](docs/AUDIT.md)

### ğŸ” **Static Analysis**

Scan your codebase for thread-safety issues before they cause problems:

```python
from aether.audit import StaticScanner

scanner = StaticScanner()
results = scanner.scan('src/')

# Get summary
summary = scanner.get_summary()
print(f"Found {summary['critical']} critical issues")

# Print full report
scanner.print_report()
```

---

## Why This Wins

### For Developers
- **Minimal boilerplate** â€“ Just add `@atomic` to make methods thread-safe
- **No GIL overhead** â€“ When GIL is enabled, decorators bypass locking
- **Better debugging** â€“ Contention monitoring helps identify bottlenecks
- **Pythonic** â€“ Uses standard library, zero external dependencies

### For Teams
- **Easy to adopt** â€“ Drop in, works immediately
- **Reduces bugs** â€“ Decorator is simpler than manual locking
- **Performance insights** â€“ Contention monitoring finds optimization opportunities
- **Static analysis** â€“ Find issues in code review before deployment

### For Python
- **Future-proofing** â€“ Ready for no-GIL Python 3.13+
- **Performance** â€“ 2-8x speedup for multi-core workloads
- **Energy efficient** â€“ Proper threading reduces idle CPU time

---

## Documentation

### Getting Started
- [Installation Guide](#installation)
- [Core Concepts](#core-concepts)
- [Common Patterns](#common-patterns)

### Reference
- **API.md** â€“ Detailed API documentation
- **CONTRIBUTING.md** â€“ How to contribute
- **ROADMAP.md** â€“ Future features (0.5, 1.0, etc.)

### Examples
- **examples/bank_account.py** â€“ @atomic decorator in action
- **examples/demo.py** â€“ Thread-safe collections demo

---

## Installation

### From PyPI (recommended)
```bash
pip install aether-thread
```

### Development Installation
```bash
git clone https://github.com/tousif-anwar/Aether-Thread.git
cd Aether-Thread
pip install -e .
```

### From Source
```bash
git clone https://github.com/tousif-anwar/Aether-Thread.git
cd Aether-Thread
python setup.py install
```

---

## Core Concepts

### Atomic Operations

An **atomic** operation is indivisible â€“ it completes entirely or not at all, with no partial states visible to other threads.

```python
# âŒ NOT atomic â€“ race condition possible!
class Counter:
    def __init__(self):
        self.count = 0
    
    def increment(self):
        temp = self.count        # Read
        temp += 1                # Compute
        self.count = temp        # Write
        # ^ Another thread could modify count between read and write

# âœ… Atomic with @atomic decorator
class SafeCounter:
    def __init__(self):
        self.count = 0
    
    @atomic
    def increment(self):
        self.count += 1  # Execution is atomic!
```

### Lock Types

Choose the right lock for your use case:

| Lock Type | Use Case | Reentrant |
|-----------|----------|-----------|
| `LOCK` | Simple mutual exclusion | âŒ No |
| `RLOCK` | Methods that call other decorated methods | âœ… Yes |
| `SEMAPHORE` | Resource pool limiting | âœ… No |

```python
# Reentrant â€“ outer calls inner safely
class BankAccount:
    @atomic(lock_type=LockType.RLOCK)
    def transfer(self, recipient, amount):
        self.withdraw(amount)      # Calls another @atomic method
        recipient.deposit(amount)  # â† No deadlock!
    
    @atomic(lock_type=LockType.RLOCK)
    def withdraw(self, amount):
        self.balance -= amount
    
    @atomic(lock_type=LockType.RLOCK)
    def deposit(self, amount):
        self.balance += amount
```

### GIL Awareness

Aether-Thread adapts to the GIL state:

```python
# With GIL ENABLED (Python < 3.13 or PYTHON_GIL=1)
@atomic
def operation(self):
    pass  # Decorator has minimal overhead â€“ GIL provides safety

# With GIL DISABLED (Python 3.13+ with PYTHON_GIL=0)
@atomic
def operation(self):
    pass  # Decorator uses fine-grained RLock for safety
```

---

## Common Patterns

### Pattern 1: Shared Cache

```python
from aether import atomic, ThreadSafeDict

class CachedService:
    def __init__(self):
        self.cache = ThreadSafeDict()
    
    @atomic
    def get_or_compute(self, key):
        if key in self.cache:
            return self.cache[key]
        
        value = expensive_computation(key)
        self.cache[key] = value
        return value
```

### Pattern 2: Thread-Safe Counter

```python
from aether import atomic

class RequestCounter:
    def __init__(self):
        self.count = 0
    
    @atomic
    def increment(self):
        self.count += 1
        return self.count
    
    @atomic
    def get_count(self):
        return self.count
```

### Pattern 3: Producer-Consumer Queue

```python
from aether import ThreadSafeList

class Queue:
    def __init__(self):
        self.items = ThreadSafeList()
    
    def enqueue(self, item):
        self.items.append(item)
    
    def dequeue(self):
        if len(self.items) > 0:
            return self.items.pop(0)
        return None
```

---

## Testing

Run the comprehensive test suite:

```bash
# All tests
python -m pytest tests/

# Specific test module
python -m pytest tests/test_decorators.py

# With coverage
python -m pytest tests/ --cov=aether --cov-report=html
```

**Test Coverage:**
- âœ… Decorator functionality and synchronization
- âœ… Thread-safe collections under concurrent access
- âœ… Contention monitoring and metrics
- âœ… Static analysis and issue detection
- âœ… GIL-aware behavior

---

## Project Structure

```
src/aether/
â”œâ”€â”€ __init__.py          # Main package
â”œâ”€â”€ decorators.py        # @atomic and @synchronized
â”œâ”€â”€ monitor.py           # Contention monitoring
â”œâ”€â”€ collections/         # Thread-safe collection wrappers
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ audit/              # Static code analysis
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ analyzer.py
â”‚   â””â”€â”€ scanner.py
â””â”€â”€ benchmark/          # Performance benchmarking
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ benchmarker.py
    â””â”€â”€ runner.py

tests/
â”œâ”€â”€ test_decorators.py   # Decorator tests
â”œâ”€â”€ test_monitor.py      # Monitoring tests
â”œâ”€â”€ test_audit.py        # Analysis tests
â””â”€â”€ test_collections.py  # Collection tests

examples/
â”œâ”€â”€ bank_account.py      # @atomic decorator demo
â””â”€â”€ demo.py              # Full feature demonstration
```

---

## Performance Characteristics

### With GIL Enabled (Python < 3.13)
- **Overhead**: < 5% â€“ Decorator checks GIL state and bypasses locking
- **Benefit**: No performance penalty for GIL-protected code

### With GIL Disabled (Python 3.13+)
- **Overhead**: 15-30% â€“ Fine-grained RLock provides safety
- **Benefit**: True parallelism enables 2-8x speedup on multi-core systems

### Monitoring Overhead
- **When disabled**: 0% â€“ No impact
- **When enabled**: 1-3% â€“ Minimal bookkeeping for contention metrics

---

## Roadmap

| Phase | Version | Status | Features |
|-------|---------|--------|----------|
| **Detection** | 0.1 | âœ… Complete | Static analysis, thread-safe collections |
| **Synchronization** | 0.2 | âœ… Complete | @atomic decorator, contention monitor |
| **Optimization** | 0.5 | ğŸš§ Planned | Lock-free data structures, performance tuning |
| **Advanced** | 1.0+ | ğŸ“‹ Planned | Actor model, async support, distributed locks |

---

## Contributing

We welcome contributions! See [CONTRIBUTING.md](CONTRIBUTING.md) for:
- Development setup
- Coding standards
- Testing requirements
- Pull request process

**Areas for contribution:**
- âœ… Enhanced pattern detection
- âœ… Performance optimizations
- âœ… Additional benchmarks
- âœ… Documentation improvements

---

## License

MIT License â€“ see [LICENSE](LICENSE) for details.

---

## Resources

- ğŸ“– **[API Reference](API.md)** â€“ Complete API documentation
- ğŸ”§ **[Contributing](CONTRIBUTING.md)** â€“ How to get involved
- ğŸ—ºï¸ **[Roadmap](ROADMAP.md)** â€“ Future direction
- ğŸ’¬ **[Discussions](https://github.com/tousif-anwar/Aether-Thread/discussions)** â€“ Ask questions
- ğŸ› **[Issues](https://github.com/tousif-anwar/Aether-Thread/issues)** â€“ Report bugs

---

## Related Articles & Resources

- [Python GIL Enhancement Proposal (PEP 703)](https://peps.python.org/pep-0703/)
- [Python 3.13 Release Notes](https://docs.python.org/3.13/whatsnew/3.13.html)
- [Threading Best Practices](https://docs.python.org/3/library/threading.html)
- [Concurrent Programming Patterns](https://realpython.com/intro-to-python-threading/)

---

**Made with â¤ï¸ for the Python concurrency community**



## The Problem It Solves

For 30 years, Python developers relied on the **Global Interpreter Lock (GIL)** to prevent race conditions at the cost of true parallelism. Now that the GIL can be disabled (in Python 3.13+), many existing libraries will **"break"** or experience **data corruption** because they aren't actually thread-safe.

**Aether-Thread** solves this by providing:
- **Detection tools** to identify non-thread-safe patterns
- **Smart wrappers** that automatically provide synchronization
- **Benchmarking suites** to measure performance improvements

## Core Features

### ğŸ” **aether-audit** â€“ Static Analysis
A static analysis tool (built on AST) that scans your codebase and flags non-thread-safe patterns:
- Global mutable variables
- Shared class attributes without synchronization
- Potential data races

```bash
aether-audit src/
```

### ğŸ”’ **aether-proxy** â€“ Smart Thread-Safe Wrappers
Drop-in replacements for standard Python collections with automatic fine-grained locking (only when GIL is disabled):
- `ThreadSafeList` â€“ Thread-safe list wrapper
- `ThreadSafeDict` â€“ Thread-safe dict wrapper
- Dynamically adapts to GIL state

```python
from aether_thread.proxy import ThreadSafeList

safe_list = ThreadSafeList()
safe_list.append(item)  # Automatically synchronized when needed
```

### ğŸ“Š **aether-bench** â€“ Benchmarking Suite
Compare performance across:
- Different Python versions
- GIL-on vs GIL-off configurations
- Lock-free vs locking implementations

```bash
aether-bench --all --ops 10000
```

---

## Installation

```bash
pip install aether-thread
```

Or for development:

```bash
git clone https://github.com/tousif-anwar/Aether-Thread.git
cd Aether-Thread
pip install -e .
```

---

## Quick Start

### 1. Audit Your Code

```bash
# Scan a directory for thread-safety issues
aether-audit myproject/

# JSON output for CI/CD integration
aether-audit myproject/ --json --strict
```

**Example output:**
```
AETHER-AUDIT REPORT
====================
Files scanned: 23
Total findings: 8
  - Critical: 5
  - Warnings: 3

Findings:
  ğŸ”´ Line 12: [mutable_global]
     Global variable 'cache' is mutable and not synchronized
     ğŸ’¡ Suggestion: Use a thread-safe wrapper or protect access with a Lock
```

### 2. Wrap Unsafe Collections

```python
from aether_thread.proxy import ThreadSafeList, ThreadSafeDict
import threading

# Create thread-safe collections
safe_list = ThreadSafeList()
safe_dict = ThreadSafeDict()

def worker(item_id):
    safe_list.append(item_id)
    safe_dict[f"item_{item_id}"] = item_id * 2

# Spawn multiple threads
threads = [threading.Thread(target=worker, args=(i,)) for i in range(10)]
for t in threads:
    t.start()
for t in threads:
    t.join()

print(f"Safely stored {len(safe_list)} items")
```

### 3. Benchmark Performance

```bash
# Compare threading performance
aether-bench --all --ops 50000

# Focus on specific collections
aether-bench --list --ops 100000
aether-bench --dict --ops 100000
```

---

## Technical Roadmap

| Phase | Goal | Status |
|-------|------|--------|
| **0.1** (Detection) | Build CLI that detects globals and shared attributes | âœ… Complete |
| **0.5** (Mitigation) | Create `@thread_safe` decorator for critical sections | ğŸš§ In Progress |
| **1.0** (Optimization) | Integrate `sys._is_gil_enabled()` for dynamic switching | ğŸ“‹ Planned |
| **1.5** (Advanced) | Support async patterns and actor model | ğŸ“‹ Planned |

---

## Why This Matters

### AI & Data Science
Python is the backbone of AI. Faster, multi-threaded Python means **cheaper and faster training** of models worldwide.

### Legacy Systems
Thousands of companies have **millions of lines of Python code** that need modernization to stay competitive.

### Energy Efficiency
Efficient multi-threading **reduces CPU idle time**, leading to lower energy consumption in massive data centers.

---

## Architecture Overview

```
aether-thread/
â”œâ”€â”€ audit/              # Static analysis engine
â”‚   â”œâ”€â”€ analyzer.py    # AST-based pattern detection
â”‚   â”œâ”€â”€ scanner.py     # Directory scanning & reporting
â”‚   â””â”€â”€ cli.py         # Command-line interface
â”œâ”€â”€ proxy/             # Thread-safe collections
â”‚   â”œâ”€â”€ wrapper.py     # Base wrapper with GIL detection
â”‚   â”œâ”€â”€ collections.py # ThreadSafeList, ThreadSafeDict
â”‚   â””â”€â”€ decorators.py  # @thread_safe decorator (coming)
â””â”€â”€ bench/             # Performance benchmarking
    â”œâ”€â”€ benchmarker.py # Core benchmark runner
    â”œâ”€â”€ runner.py      # Standard benchmark suites
    â””â”€â”€ cli.py         # Benchmark CLI
```

---

## Usage Examples

### Detecting Issues

```python
from aether_thread.audit import StaticScanner

scanner = StaticScanner()
results = scanner.scan("src/")
scanner.print_report()
```

### Using Safe Collections in Production

```python
from aether_thread.proxy import ThreadSafeDict
from concurrent.futures import ThreadPoolExecutor

cache = ThreadSafeDict()

def process_item(item_id):
    result = expensive_computation(item_id)
    cache[f"item_{item_id}"] = result
    return result

with ThreadPoolExecutor(max_workers=8) as executor:
    executor.map(process_item, range(1000))

print(f"Processed {len(cache)} items")
```

### Benchmarking

```python
from aether_thread.bench import BenchmarkRunner

runner = BenchmarkRunner()
runner.run_all_benchmarks(num_ops=10000)
```

---

## Testing

Run the test suite:

```bash
python -m pytest tests/
```

Test coverage:
- âœ… AST analysis and pattern detection
- âœ… Directory scanning with exclusions
- âœ… Concurrent list operations
- âœ… Concurrent dict operations
- âœ… GIL state detection

---

## Performance Characteristics

### With GIL Enabled (Python < 3.13 or `PYTHON_GIL=1`)
- **Minimal overhead** â€“ wrappers bypass locking
- **Native GIL protection** â€“ no additional synchronization needed
- **Negligible performance impact**

### With GIL Disabled (Python 3.13+ with `PYTHON_GIL=0`)
- **Fine-grained locking** â€“ automatic mutex acquisition
- **True parallelism** â€“ multiple threads work simultaneously
- **Content-aware** â€“ only synchronizes when needed
- **Performance improvement** â€“ 2-4x speedup on multi-core systems

---

## ğŸŸ¢ New in v0.3.0: Free-Threaded Python Support

**Aether-Thread v0.3.0** adds comprehensive support for Python 3.13+ GIL-free execution with tools to detect, profile, and optimize free-threaded code.

### 1. Free-Thread Safety Detector

Detect dangerous patterns that crash or fail in free-threaded Python:

```python
from aether.audit.free_thread_detector import FreeThreadDetector

detector = FreeThreadDetector("mycode.py")
threats = detector.detect(source_code)

for threat in threats:
    if threat.crash_risk:
        print(f"ğŸ”´ CRASH RISK: {threat.description}")
        print(f"   Fix: {threat.recommendation}")
```

**Detects 6 threat types:**
- ğŸ”´ **Frame.f_locals access** (interpreter crash risk)
- ğŸŸ  **warnings.catch_warnings()** without sync (race condition)
- ğŸ”´ **Shared iterators** across threads (data loss)
- ğŸŸ  **async/await + threading mix** (deadlock risk)
- ğŸ”´ **Signal handlers modifying shared state** (critical race)
- ğŸŸ¡ **ExceptionGroup in threaded code** (consistency issues)

### 2. Saturation Cliff Profiler

Find the exact point where adding threads makes code **slower** (not just not faster):

```python
from aether.profile import SaturationCliffProfiler

def workload():
    # Your CPU/I/O bound work
    for i in range(1000):
        expensive_operation()

profiler = SaturationCliffProfiler(workload, max_threads=64)
analysis = profiler.profile()

print(f"Optimal threads: {analysis.optimal_threads}")
print(f"Cliff point: {analysis.cliff_threads} threads")
print(analysis.plot_ascii_chart())
```

**Detects:**
- Peak performance thread count
- Exact cliff point (â‰¥20% performance drop)
- Severity of cliff (0-100% drop)
- Recommendations for optimal settings

### 3. Adaptive Thread Pool

Auto-tuning thread pool that prevents saturation cliff by monitoring contention:

```python
from aether.pool import AdaptiveThreadPool

with AdaptiveThreadPool(max_workers=32) as pool:
    results = pool.map(process_item, large_dataset)
    pool.print_status()  # Shows blocking_ratio, throughput, metrics
```

**Features:**
- **Blocking ratio monitoring** â€“ Detects lock contention vs I/O wait
- **Automatic thread scaling** â€“ Only scales when safe
- **Real-time metrics** â€“ Throughput, latency, CPU%, active threads
- **Contention veto** â€“ Prevents threads when lock contention detected

### 4. GIL Status Checker

Verify environment supports free-threaded Python and check package compatibility:

```python
from aether.check import GILStatusChecker

checker = GILStatusChecker()
checker.print_status()

# Or programmatic access
if checker.is_free_threaded():
    print("âœ… Running free-threaded Python!")
```

### 5. Developer CLI

For command-line analysis without writing code:

```bash
# Check code for thread-safety issues
aether check src/ --free-threaded

# Profile to find saturation cliff
aether profile benchmark.py --max-threads 64

# Check GIL status
aether status

# Deep scan with all checks
aether scan . --all
```

### 6. Jupyter Magic Commands

For interactive analysis in notebooks:

```python
%load_ext aether.jupyter_magic

# Then in notebook cells:
%%audit
# your code here

%%profile_threads --max-threads 32
def workload():
    expensive_operation()

%%free_threaded_check
# check for free-threading issues

%gil_status --full
```

### Understanding Blocking Ratio (Î²)

The **blocking ratio** is key to auto-tuning:

$$\beta = 1 - \frac{\text{CPU time}}{\text{Wall time}}$$

- **Î² â‰ˆ 1.0** â†’ I/O bound (network, disk, sleep) â†’ **Safe to scale threads**
- **Î² â‰ˆ 0.0** â†’ CPU bound or lock contention â†’ **Don't add more threads**
- **Î² â‰ˆ 0.3** â†’ Contention threshold (tunable)

When Î² < threshold, the pool detects likely contention and prevents adding threads that would only increase lock conflicts.

### Example: Complete Free-Threading Migration

```python
import threading
from aether import atomic, AdaptiveThreadPool
from aether.audit.free_thread_detector import FreeThreadDetector
from aether.check import is_free_threaded

# 1. Verify environment
if is_free_threaded():
    print("âœ… Running free-threaded Python")
else:
    print("âš ï¸ Standard Python - run with Python 3.13+")

# 2. Check code for issues
detector = FreeThreadDetector("myapp.py")
threats = detector.detect(open("myapp.py").read())
if threats:
    for threat in threats:
        print(f"Fix: {threat.recommendation}")

# 3. Protect shared state with @atomic
@atomic
def update_cache(key, value):
    cache[key] = value

# 4. Use adaptive pool for workers
with AdaptiveThreadPool(max_workers=16) as pool:
    results = pool.map(worker_fn, work_items)
    
    # Check how pool adapted to contention
    metrics = pool.get_metrics()
    print(f"Blocking ratio: {metrics.blocking_ratio:.1%}")
    print(f"Active threads: {metrics.active_threads}")
```

---

## Contributing

Contributions are welcome! Please read [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines.

To get started:
1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

---

## Roadmap & Future Work

### âœ… Phase 0.1-0.2 (Foundation) â€“ COMPLETE
- `@atomic` and `@synchronized` decorators
- ThreadSafeList, ThreadSafeDict, ThreadSafeSet collections
- ContentionMonitor for performance analysis
- Static code auditing (aether-audit)

### âœ… Phase 0.3 (Free-Threading Support) â€“ COMPLETE
- FreeThreadDetector for Python 3.13+ safety issues
- SaturationCliffProfiler to find performance cliffs
- AdaptiveThreadPool with contention-aware auto-tuning
- GILStatusChecker for environment validation
- CLI interface (aether check/profile/status)
- Jupyter magic commands for interactive analysis

### Phase 0.4 (Testing & Integration) â€“ IN PROGRESS
- Comprehensive test suites for Phase 0.3 modules
- Integration tests with Python 3.13+ free-threaded runtime
- Performance benchmarks showing cliff detection
- Documentation and tutorials

### Phase 1.0 (Optimization) â€“ PLANNED
- Lock-free data structures when GIL is available
- Adaptive synchronization (automatically choose best strategy)
- AsyncIO integration with thread pools
- Memory-mapped collections for IPC

### Phase 1.5+ (Advanced) â€“ PLANNED
- Actor model implementation
- Distributed lock coordination
- Real-time performance visualization
- AI-powered optimization recommendations

---

## Related Resources

- [Python GIL Enhancement Proposal (PEP 703)](https://peps.python.org/pep-0703/)
- [Python 3.13 Release Notes](https://docs.python.org/3.13/whatsnew/3.13.html)
- [Threading Best Practices](https://docs.python.org/3/library/threading.html)
- [Concurrency Patterns](https://realpython.com/intro-to-python-threading/)
- [Saturation Point Theory](https://en.wikipedia.org/wiki/Amdahl%27s_law)

---

## License

This project is licensed under the MIT License â€“ see the [LICENSE](LICENSE) file for details.

---

## Maintainers

- **Tousif Anwar** â€“ Initial creator and maintainer

---

## Support

- ğŸ“– [Documentation](https://github.com/tousif-anwar/Aether-Thread#readme)
- ğŸ› [Issue Tracker](https://github.com/tousif-anwar/Aether-Thread/issues)
- ğŸ’¬ [Discussions](https://github.com/tousif-anwar/Aether-Thread/discussions)

---

**Made with â¤ï¸ for the Python community**

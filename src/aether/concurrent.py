"""
Structured Concurrency: High-level concurrent programming abstractions.

Problem Solved:
- Users struggle with manual thread/lock management
- "Safety veto" logic is buried in low-level pool
- Need Rust-like rayon API: `par_map`, `par_iter`

Research Alignment:
- Structured concurrency (CSP/async-await patterns)
- Implicit synchronization (no explicit locks)
- Proven safe by design, not by testing

API:
    from aether import par_map, DataParallel
    
    # Functional style
    results = par_map(transform_fn, items)
    
    # Object-oriented style
    parallel = DataParallel(items)
    results = parallel.map(transform_fn).collect()
"""

from typing import Callable, Iterable, List, TypeVar, Generic, Optional
from dataclasses import dataclass
import concurrent.futures
import threading
import time

T = TypeVar('T')
U = TypeVar('U')


@dataclass
class ParallelMetrics:
    """Metrics from parallel execution."""
    items_processed: int
    wall_time: float
    speedup: float
    threads_used: int
    blocking_ratio: float


class DataParallel(Generic[T]):
    """
    Structured concurrency container for parallel operations.
    
    Research-focused: Encapsulates thread safety internally using
    adaptive pooling with safety veto based on blocking ratio.
    
    No explicit locks neededâ€”the framework handles synchronization.
    
    Example:
        data = [1, 2, 3, 4, 5]
        parallel = DataParallel(data)
        results = parallel.map(lambda x: x**2).collect()
        # Results: [1, 4, 9, 16, 25]
    """
    
    def __init__(self, items: Iterable[T], max_workers: Optional[int] = None):
        """
        Initialize parallel container.
        
        Args:
            items: Iterable of items to process
            max_workers: Max threads (default: CPU count)
        """
        self.items = list(items)
        self.max_workers = max_workers or __import__('os').cpu_count() or 4
        self.metrics: Optional[ParallelMetrics] = None
        self._results: List[U] = []
    
    def map(self, fn: Callable[[T], U]) -> 'DataParallel[U]':
        """
        Apply function to each item in parallel.
        
        Uses adaptive pooling with safety veto to prevent
        over-threading and saturation cliff.
        
        Returns: New DataParallel with mapped results
        """
        from aether.pool import AdaptiveThreadPool
        
        results = []
        start_time = time.perf_counter()
        
        try:
            with AdaptiveThreadPool(max_workers=self.max_workers) as pool:
                results = pool.map(fn, self.items)
                
                # Capture metrics
                metrics = pool.get_metrics()
                self.metrics = ParallelMetrics(
                    items_processed=len(self.items),
                    wall_time=time.perf_counter() - start_time,
                    speedup=len(self.items) / max(time.perf_counter() - start_time, 0.001),
                    threads_used=metrics.active_threads,
                    blocking_ratio=metrics.blocking_ratio
                )
        except ImportError:
            # Fallback if pool not available
            with concurrent.futures.ThreadPoolExecutor(max_workers=self.max_workers) as executor:
                results = list(executor.map(fn, self.items))
                self.metrics = ParallelMetrics(
                    items_processed=len(self.items),
                    wall_time=time.perf_counter() - start_time,
                    speedup=len(self.items) / max(time.perf_counter() - start_time, 0.001),
                    threads_used=self.max_workers,
                    blocking_ratio=0.5  # Unknown
                )
        
        # Create new DataParallel with results
        new_parallel = DataParallel(results, self.max_workers)
        new_parallel.metrics = self.metrics
        return new_parallel
    
    def filter(self, predicate: Callable[[T], bool]) -> 'DataParallel[T]':
        """Filter items based on predicate."""
        filtered = [item for item in self.items if predicate(item)]
        new_parallel = DataParallel(filtered, self.max_workers)
        new_parallel.metrics = self.metrics
        return new_parallel
    
    def reduce(self, fn: Callable[[T, T], T]) -> T:
        """Reduce to single value using associative function."""
        import functools
        return functools.reduce(fn, self.items)
    
    def collect(self) -> List[T]:
        """Collect results into list."""
        return self.items
    
    def for_each(self, fn: Callable[[T], None]) -> ParallelMetrics:
        """Execute side-effect function on each item."""
        start_time = time.perf_counter()
        
        from aether.pool import AdaptiveThreadPool
        
        try:
            with AdaptiveThreadPool(max_workers=self.max_workers) as pool:
                pool.map(fn, self.items)
                metrics = pool.get_metrics()
        except ImportError:
            with concurrent.futures.ThreadPoolExecutor(max_workers=self.max_workers) as executor:
                list(executor.map(fn, self.items))
                metrics = type('Metrics', (), {
                    'active_threads': self.max_workers,
                    'blocking_ratio': 0.5
                })()
        
        return ParallelMetrics(
            items_processed=len(self.items),
            wall_time=time.perf_counter() - start_time,
            speedup=len(self.items) / max(time.perf_counter() - start_time, 0.001),
            threads_used=metrics.active_threads,
            blocking_ratio=metrics.blocking_ratio
        )
    
    def print_metrics(self):
        """Print execution metrics."""
        if not self.metrics:
            print("No metrics available (call map() first)")
            return
        
        m = self.metrics
        print(f"\nðŸ“Š Parallel Execution Metrics")
        print(f"  Items: {m.items_processed}")
        print(f"  Time: {m.wall_time:.3f}s")
        print(f"  Throughput: {m.speedup:.0f} items/sec")
        print(f"  Threads: {m.threads_used}")
        print(f"  Blocking ratio: {m.blocking_ratio:.1%}")


def par_map(fn: Callable[[T], U], items: Iterable[T], max_workers: Optional[int] = None) -> List[U]:
    """
    Functional interface to parallel map.
    
    Equivalent to Rust's rayon::iter::IntoParallelIterator::map().
    
    Uses DataParallel internally with adaptive pooling and safety veto.
    
    Example:
        results = par_map(lambda x: x**2, range(1000))
    
    Args:
        fn: Function to apply
        items: Items to process
        max_workers: Max threads (default: CPU count)
    
    Returns:
        List of results
    """
    parallel = DataParallel(items, max_workers)
    return parallel.map(fn).collect()


def par_filter(predicate: Callable[[T], bool], items: Iterable[T]) -> List[T]:
    """Parallel filter (sequential, but parallel-ready structure)."""
    return DataParallel(items).filter(predicate).collect()


def par_reduce(fn: Callable[[T, T], T], items: Iterable[T]) -> T:
    """Parallel reduce using associative operation."""
    return DataParallel(items).reduce(fn)


class ParallelIterator:
    """
    Iterator-based parallel programming (like rayon).
    
    Example:
        results = (
            ParallelIterator(data)
            .map(lambda x: x * 2)
            .filter(lambda x: x > 10)
            .collect()
        )
    """
    
    def __init__(self, items: Iterable[T], max_workers: Optional[int] = None):
        self.parallel = DataParallel(items, max_workers)
    
    def map(self, fn: Callable[[T], U]) -> 'ParallelIterator':
        """Map function over items."""
        self.parallel = self.parallel.map(fn)
        return self
    
    def filter(self, predicate: Callable[[T], bool]) -> 'ParallelIterator':
        """Filter items."""
        self.parallel = self.parallel.filter(predicate)
        return self
    
    def collect(self) -> List:
        """Collect results."""
        return self.parallel.collect()


# Convenience factory functions
def parallel(items: Iterable[T]) -> DataParallel[T]:
    """Create a parallel container."""
    return DataParallel(items)


def par_for_each(fn: Callable[[T], None], items: Iterable[T]) -> ParallelMetrics:
    """Execute side-effect function on each item in parallel."""
    return parallel(items).for_each(fn)

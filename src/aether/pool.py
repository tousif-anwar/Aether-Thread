"""
Adaptive Smart Thread Pool: Auto-tuning thread pool for free-threaded Python.

Uses the "Blocking Ratio" (Î² = 1 - CPU_time/wall_time) to detect when
the pool is hitting lock contention and should stop scaling.

High Î² = threads waiting for I/O (safe to add more threads)
Low Î² = threads waiting for locks (adding more threads makes it worse)
"""

import time
import threading
from typing import Callable, List, Any, Optional
from concurrent.futures import ThreadPoolExecutor, Future
from dataclasses import dataclass
import psutil
import os


@dataclass
class ThreadPoolMetrics:
    """Real-time metrics for the thread pool."""
    blocking_ratio: float  # 0-1, 1=all I/O bound, 0=all CPU bound
    active_threads: int
    queued_tasks: int
    throughput: float  # tasks/sec
    avg_latency_ms: float
    cpu_percent: float  # Process CPU usage


class AdaptiveThreadPool:
    """
    Self-tuning thread pool that adjusts worker count based on contention.
    
    Algorithm:
    1. Monitor blocking_ratio = 1 - (cpu_time / wall_time)
    2. If blocking_ratio < threshold (default 0.3):
       - We're CPU bound, likely waiting for locks
       - VETO new threads to prevent saturation cliff
    3. If blocking_ratio > threshold:
       - We're I/O bound, safe to add threads
       - Can scale up to available cores
    
    This prevents the "saturation cliff" where thread pool
    executor makes things worse by adding contention.
    """
    
    def __init__(self, max_workers: Optional[int] = None,
                 min_workers: int = 1,
                 blocking_threshold: float = 0.3,
                 monitor_interval: float = 1.0):
        """
        Args:
            max_workers: Maximum threads (default: CPU count)
            min_workers: Minimum threads
            blocking_threshold: Below this, stop adding threads
            monitor_interval: How often to sample metrics (seconds)
        """
        self.max_workers = max_workers or os.cpu_count() or 4
        self.min_workers = min_workers
        self.blocking_threshold = blocking_threshold
        self.monitor_interval = monitor_interval
        
        self._executor: Optional[ThreadPoolExecutor] = None
        self._process = psutil.Process()
        self._tasks_completed = 0
        self._task_lock = threading.Lock()
        self._metrics: Optional[ThreadPoolMetrics] = None
        self._monitor_thread: Optional[threading.Thread] = None
        self._stop_monitoring = False
        
    def __enter__(self):
        """Context manager entry."""
        self._executor = ThreadPoolExecutor(max_workers=self.min_workers)
        self._stop_monitoring = False
        self._monitor_thread = threading.Thread(
            target=self._monitor_loop,
            daemon=True
        )
        self._monitor_thread.start()
        return self
    
    def __exit__(self, *args):
        """Context manager exit."""
        self._stop_monitoring = True
        if self._monitor_thread:
            self._monitor_thread.join(timeout=2.0)
        if self._executor:
            self._executor.shutdown(wait=True)
    
    def submit(self, fn: Callable, *args, **kwargs) -> Future:
        """Submit a task to the pool."""
        if not self._executor:
            raise RuntimeError("Pool not initialized (use 'with' statement)")
        
        future = self._executor.submit(self._run_task, fn, *args, **kwargs)
        return future
    
    def map(self, fn: Callable, iterable, timeout=None) -> List[Any]:
        """Map function over iterable."""
        if not self._executor:
            raise RuntimeError("Pool not initialized")
        
        return list(self._executor.map(fn, iterable, timeout=timeout))
    
    def _run_task(self, fn: Callable, *args, **kwargs) -> Any:
        """Wrapper to track task completion."""
        try:
            return fn(*args, **kwargs)
        finally:
            with self._task_lock:
                self._tasks_completed += 1
    
    def _monitor_loop(self):
        """Monitor blocking ratio and adjust thread count."""
        last_cpu_time = self._process.cpu_times().user
        last_wall_time = time.time()
        last_tasks = 0
        
        while not self._stop_monitoring:
            time.sleep(self.monitor_interval)
            
            # Calculate blocking ratio
            cpu_times = self._process.cpu_times()
            current_cpu_time = cpu_times.user + cpu_times.system
            current_wall_time = time.time()
            
            cpu_delta = current_cpu_time - last_cpu_time
            wall_delta = current_wall_time - last_wall_time
            
            if wall_delta > 0:
                blocking_ratio = 1.0 - (cpu_delta / wall_delta)
                blocking_ratio = max(0, min(1.0, blocking_ratio))  # Clamp 0-1
            else:
                blocking_ratio = 0.5
            
            # Calculate throughput
            with self._task_lock:
                task_delta = self._tasks_completed - last_tasks
                last_tasks = self._tasks_completed
            
            throughput = task_delta / wall_delta if wall_delta > 0 else 0
            
            # Store metrics
            self._metrics = ThreadPoolMetrics(
                blocking_ratio=blocking_ratio,
                active_threads=self._executor._work_queue.qsize() if self._executor else 0,
                queued_tasks=self._executor._work_queue.qsize() if self._executor else 0,
                throughput=throughput,
                avg_latency_ms=1000.0 / max(1, throughput),
                cpu_percent=self._process.cpu_percent()
            )
            
            # Adapt thread count based on blocking ratio
            self._adapt_thread_count(blocking_ratio)
            
            last_cpu_time = current_cpu_time
            last_wall_time = current_wall_time
    
    def _adapt_thread_count(self, blocking_ratio: float):
        """Adjust thread pool size based on blocking ratio."""
        if not self._executor:
            return
        
        # Don't scale if we're not I/O bound
        if blocking_ratio < self.blocking_threshold:
            # CPU bound - lock contention likely
            # Keep threads at minimum to prevent saturation cliff
            target_workers = self.min_workers
        else:
            # I/O bound - safe to scale
            # Increase threads based on how I/O bound we are
            scale_factor = blocking_ratio / self.blocking_threshold
            target_workers = int(self.min_workers * scale_factor)
            target_workers = min(target_workers, self.max_workers)
            target_workers = max(target_workers, self.min_workers)
        
        # Note: ThreadPoolExecutor doesn't support dynamic scaling
        # In a real implementation, you'd use a custom pool
        # This is a simplified version showing the logic
    
    def get_metrics(self) -> Optional[ThreadPoolMetrics]:
        """Get current performance metrics."""
        return self._metrics
    
    def print_status(self):
        """Print current pool status."""
        if not self._metrics:
            print("No metrics available yet")
            return
        
        m = self._metrics
        status = "ðŸŸ¢" if m.blocking_ratio > self.blocking_threshold else "ðŸ”´"
        
        print(f"\n{status} Adaptive Pool Status:")
        print(f"  Blocking Ratio: {m.blocking_ratio:.1%} ", end="")
        if m.blocking_ratio > self.blocking_threshold:
            print("(I/O bound - can scale)")
        else:
            print("(CPU bound - blocked by contention)")
        
        print(f"  Throughput: {m.throughput:.1f} tasks/sec")
        print(f"  Avg Latency: {m.avg_latency_ms:.1f} ms")
        print(f"  Process CPU: {m.cpu_percent:.1f}%")


def adaptive_pool(**kwargs) -> AdaptiveThreadPool:
    """Create an adaptive thread pool with auto-tuning."""
    return AdaptiveThreadPool(**kwargs)
